# Ensembling Methods

Ensembling methods in statistics refer to combining the predictions of multiple models to produce a more accurate and robust prediction.
Ensembling methods have been shown to be highly effective in a wide range of applications, including:
- Image recognition
- Speech recognition
- Natural language processing
- Anomaly detection
- Recommendation systems

They are particularly useful when working with complex, high-dimensional data, where individual models may struggle to capture all the relevant information
## Bagging

Bagging methods involve training multiple models independently and combining their predictions by taking a simple average or majority vote.

An example of bagging methods is the famous Random forests.


## Boosting

Boosting methods involve iteratively training models on the data, where each subsequent model tries to correct the errors of the previous one.

Examples of boosting methods include:
- AdaBoost
- Gradient boosting


#  Algorithms in this section

## Bagging
 TODO: [Algorithm description]
## AdaBoost
 TODO: [Algorithm description]

TODO:
* implement
    * ~~Implement geniric bagging~~
    * ~~Implement Random Forest~~
    * ~~AdaBoost (Adaptive Boosting)~~
    * Gradient Boosting
    * XGBoost (Extreme Gradient Boosting)
    * LightGBM (Light Gradient Boosting Machine)
    * CatBoost (Categorical Boosting)
